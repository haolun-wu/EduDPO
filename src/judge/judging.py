import os
import re
import json

import pandas as pd
from tqdm import tqdm
from src.Generate import add_prompt
from src.Experiment import Experiment
from huggingface_hub import ChatCompletionInputGrammarType
from dotmap import DotMap
from typing import List

from src.utils.files import load_json 



NEW2_AI_PROMPT = """
Focus on these issues only:
1) No explanation or solving the wrong problem.
2) Only an equation is provided without explanation (variable names/comments in code count as explanations).
3) Rare, but a major error in probability theory.

If no issues, give affirmative feedback (e.g., "Your learning shines!") in one phrase or sentence.

For issues, list them and provide brief corrective guidance if explanations were given.

Tone: Be a smart, kind, and insightful 20-year-old university teacher. Direct feedback to the student ("you").
"""

def generate_judging_prompt(question_text: str, ta_solution: str, stu_solution: str, feedback: str) -> str:
    prompt = f'''
    You are evaluating the quality of the feedback to a students' probability homework. 
    
    Bellow is the feedback task description given to the annotator:
    {NEW2_AI_PROMPT}
    
    Information you will also need:
    Question: {question_text}
    Teacher solution: {ta_solution}
    '''

    if stu_solution:
        prompt += f'''
    Student solution: {stu_solution}'''
        
    prompt += f'''
    Feedback: {feedback}

    Your task is to evaluate whether this feedback is correct and helpful.
    Important: Return your answer as a JSON dictionary. The dictionary should contain only two keys: "correctness" and "helpfulness".

    "correctness": true or false (whether the feedback is correct)
    "helpfulness": true or false (whether the feedback is helpful)

    '''
    
    return clean_text_formatting(prompt)


def clean_text_formatting(text: str) -> str:
    """
    Clean up text formatting by:
    1. Removing all newlines (\n)
    2. Removing extra spaces after newlines
    3. Ensuring proper spacing between sentences
    
    Args:
        text (str): The input text to clean
        
    Returns:
        str: The cleaned text with proper formatting
    """
    # First, replace all newlines with spaces
    text = text.replace('\n', ' ')
    
    # Remove multiple spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Ensure proper spacing after periods (except for decimal points)
    text = re.sub(r'\.(\d)', r'.\1', text)  # Preserve decimal points
    text = re.sub(r'\.(\s*)', '. ', text)   # Ensure space after periods
    
    # Ensure proper spacing after other sentence-ending punctuation
    text = re.sub(r'!(\s*)', '! ', text)
    text = re.sub(r'\?(\s*)', '? ', text)
    
    # Remove any remaining multiple spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Strip leading/trailing spaces
    text = text.strip()
    
    return text 



class Judge(Experiment):
    """
    Experiment class for juging the feedback 
    generated by language models. 
    """
    
    def __init__(self, config, test_run) -> None:
        super().__init__(config, test_run)

    def run(self):

        df, exists_df = self.reload_results()

        if not df.empty:
            for sub_df in self._run(self.agent, df):
                exists_df = pd.concat([sub_df, exists_df], axis=0, ignore_index=True)#.sort_values(by="id")
                exists_df = exists_df.drop(columns=[c for c in df.columns if "Unnamed:" in c])
                exists_df = exists_df.reset_index(drop=True)
                exists_df.to_csv(self.results_save_path)
                print("saving", self.results_save_path)
        else: 
            print("Nothing to judge")

    def _run(self, agent, dataframe):

        response_format = self.config.task.response_format.toDict()
        if not response_format or self.config.model.source == "local": 
            response_format = None
            extract_grading = match_criteria
        else:
            extract_grading = match_json_response
            if agent.config.source == "huggingface":
                _format = json.dumps(response_format["json_schema"]["schema"])
                response_format = ChatCompletionInputGrammarType(type="json_object", 
                                                                 value=_format)

        judge_instructions = self.config.task.instructions
        f = lambda x: add_prompt(x, judge_instructions)
        dataframe.loc[:, "judge_instructions"] = dataframe.apply(f, axis=1)
        gen_params = judge_instructions[-1]["hyperparameters"]
        gen_params["response_format"] = response_format

        groups = dataframe.groupby("id").groups
        keys = list(sorted(groups.keys()))
        all_of_them = {k: groups[k] for k in keys}

        print("Experiments to judge", dataframe.experiment.value_counts())
        
        for i, (_, index) in enumerate(all_of_them.items()):
            tqdm.pandas(desc=f'Running judging {i} / {len(all_of_them)}')
                        
            f = lambda x: str(agent.query(x, gen_params))
            results = dataframe.loc[index].judge_instructions.progress_apply(f)
            dataframe.loc[index, "judge_answer"] = list(results)

            # Extract the criteria answers
            f = lambda r: extract_grading(self.config.task.criteria, r)
            dataframe.loc[index, self.config.task.criteria] = dataframe.loc[index, "judge_answer"].apply(f)
            yield dataframe.loc[index]


    def reload_results(self):
        df = self.load_dataframe_to_judge()
        df["judge"] = self.config.name

        if True and not self.test_run and os.path.exists(self.results_save_path):
            exists_df = pd.read_csv(self.results_save_path)
            print("exists_df", exists_df)
            exists_df = exists_df.dropna(subset=self.config.task.criteria)
            
            # Need to redo that specific config 
            to_redo = ["3.json"] #["5.json", "6.json", "7.json", "8.json", "9.json", "10.json"]
            check = lambda e: True not in [r in e for r in to_redo]
            redo_mask = [check(e) for e in exists_df["experiment"]]
            exists_df = exists_df[redo_mask]

            rejected_pairs = list(zip(exists_df["id"], exists_df["experiment"]))
            mask = [False if ((i, e) in rejected_pairs) else True
                    for i, e  in list(zip(df["id"], df["experiment"]))]
            df = df.loc[mask]
        else:
            exists_df = pd.DataFrame()

        return df, exists_df


    def load_dataframe_to_judge(self):

        all_generations = []
        for subfolder in os.listdir(self.config.save_dir):
            print(subfolder)
            if not self.config.task.task_judged or self.config.task.task_judged in subfolder:
                pass 
            else:
                continue
            
            # Judging models which did a task on the same dataset 
            config_path = os.path.join(self.config.save_dir, subfolder, "experiment_configuration.json")
            if not os.path.exists(config_path): continue
        
            exp_config = DotMap(load_json(config_path))
            if exp_config.dataset.name != self.config.dataset.name:
                continue
            
            file_path = os.path.join(self.config.save_dir, subfolder, "generations.csv")
            if not os.path.exists(file_path): continue
            
            sub_df = pd.read_csv(file_path)
            all_generations.append(sub_df)

        df = pd.concat(all_generations, axis=0, ignore_index=True)

        return df 
   

def match_criteria(criteria: List[str], response: str) -> pd.Series:
    """
    Extract boolean values for grading criteria from a response string.
    
    Args:
        criteria: List of criteria to look for
        response: String containing grading criteria and values
    
    Returns:
        pandas.Series: Series containing boolean values for each criterion
    """
    results = {}
    
    # Look for the grading section
    grading_pattern = r'"grading"\s*:\s*{([^}]+)}'
    grading_match = re.search(grading_pattern, response)
    
    if grading_match:
        grading_content = grading_match.group(1)
        
        # Extract each criterion's value
        for c in criteria:
            # Pattern matches "criterion": true/false
            pattern = rf'"{c}"\s*:\s*(true|false)'
            match = re.search(pattern, grading_content, re.IGNORECASE)
            
            if match:
                value = match.group(1).lower() == 'true'
                results[c] = value
            else:
                results[c] = None
    else:
        # If no grading section found, set all criteria to None
        results = {c: None for c in criteria}
    
    return pd.Series(results)



def match_json_response(criteria, response):
    try:
        return pd.Series(json.loads(response)["grading"]) 
    except:
        return match_criteria(criteria, response)

